---
layout: post
title: Octavia--Neutron中LBaaS的参考实现
description: Octavia--Neutron中LBaaS的参考实现
category: blog
---

**声明：  
本博客欢迎转发，但请保留原作者信息!  
新浪微博：[@Lingxian_Kong](http://weibo.com/lingxiankong)   
博客地址：<http://lingxiankong.github.io/>  
联系邮箱：<anlin.kong@gmail.com>  
内容系本人学习、研究和总结，如有雷同，实属荣幸！**

目标读者：OpenStack developer or operator  
版本：Mitaka

## Octavia简介
Octavia主要参与厂商：<http://stackalytics.com/?project_type=openstack&metric=commits&module=octavia>  
Octavia的开发者文档：<http://docs.octavia.io/review/master/>

你需要了解的前置知识：

- OpenStack :-)
- Haproxy和Keeplived。Octavia说白了，就是将用户的API请求经过逻辑处理，转换成haproxy和keepalived的配置参数，下发到amphorae虚拟机中。关于haproxy和keepalived，可以详细阅读各自的官方文档。同时，可以参考如下几篇博客：

    * 介绍haproxy+keepalive安装配置，写于06 Nov 2011：<http://weizhifeng.net/HA-with-HAProxy-and-KeepAlived.html>
    * haproxy互为主备, 2013-09-29：<http://502245466.blog.51cto.com/7559397/1303506>
    * Keepalived Check and Notify Scripts, February 21, 2014: <https://tobrunet.ch/2013/07/keepalived-check-and-notify-scripts/>

- TaskFlow。Octavia的内部实现中，逻辑流程的处理主要使用TaskFlow库。关于TaskFlow，请参见[官方文档](http://docs.openstack.org/developer/taskflow/)。

## 命令行使用流程
如下几个命令创建一个loadbalancer，一个listener一个pool，同时添加两个member。

    neutron lbaas-loadbalancer-create --name lb1 private-subnet
    neutron lbaas-listener-create --loadbalancer lb1 --protocol HTTP --protocol-port 80 --name listener1
    neutron lbaas-pool-create --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTP --name pool1
    neutron lbaas-member-create  --subnet private-subnet --address ${IP1} --protocol-port 80 pool1
    neutron lbaas-member-create  --subnet private-subnet --address ${IP2} --protocol-port 80 pool1
    
## Octavia与Neutron的关系
作为Neutron中LBaaS的参考实现，与其说Octavia与Neutron的关系，还不如说Octavia与Neutron中LBaaS service plugin的关系。之前有人在网上问过同样的问题，一位Octavia core reviewer给出如下答案：

lbaas v1:  
This is the original Neutron LBaaS, and what you see in Horizon or in the neutron CLI as “lb-*”. It has an haproxy backend, and a few vendors supporting it. Feature-wise, it’s basically a byte pump.

lbaas v2:  
This is the “new” Neutron LBaaS, and is in the neutron CLI as “lbaas-*” (it’s not yet in Horizon.) It first shipped in Kilo. It re-organizes the objects, and adds TLS termination support, and has L7 plus other new goodies planned in Liberty. It similarly has an haproxy reference backend with a few vendors supporting it.

octavia:  
Think of this as a service vm framework that is specific to lbaas, to implement lbaas via nova VMs instead of “lbaas agents". It is expected to be the reference backend implementation for neutron lbaasv2 in liberty. It could also be used as its own front-end, and/or given drivers to be a load balancing framework completely outside neutron/nova, though that is not the present direction of development.

总结一下就是，Neutron中有LBaaS service plugin，在LBaaS plugin中有很多provider或者叫driver（比如开源的haproxy driver或其他很多网络设备厂家实现的driver），其中一个叫octavia driver，该driver做的事情就是调用Octavia的API，与Octavia服务交互。

还有一点值得注意，使用Octavia时，在Neutron和Octavia中都会有database，记录同样的信息。Octavia只是作为LBaaS的service provider之一，所以，Neutron DB中同样会记录loadbalancer与provider的对应关系。我问过一个core reviewer，如何保证这两份数据的一致性，他没有正面回答我，只说他也觉得这样的设计不妥，其实用Octavia一个DB即可，neutron中可以只记录mapping，但因为'political'的原因，不得不如此。因此，在生产环境，建议自己实现一致性检查。

neutron与octavia通信时，使用admin token，但会把实际租户的project id设置到操作对象结构体中，octavia会把project id记录到DB。octavia的API处理目前是没有鉴权的，生产环境部署时，建议与neutron-server部署在一起，并通过本地地址访问。

## 创建loadbalancer流程

目前支持single和active standby两种模式的loadbalancer，通过配置文件配置。测试环境可以是single，但生产环境使用时还是建议active standby。社区正在开发active active，目前（Mitaka）暂不支持。这里以active standby模式为例。

创建loadbalancer，Octavia会创建两个虚拟机。如果配置`enable_anti_affinity`，则会先在Nova创建ServerGroup（这个ServerGroup的ID会记录在DB中），两个虚拟机就会创建在不同的host上。虚拟机的flavor、image、network、keypair信息都是从配置文件中获取，其中network就是Octavia进程与虚拟机通信的管理平面。

> Octavia每创建一个loadbalancer，都会在admin租户下创建包含同样policy的ServerGroup。虚拟机以及虚拟机上的port也属于admin租户。即：创建loadbalancer的租户只能看到这个loadbalancer的信息以及loadbalancer所占用的port（VIP的port）信息，背后的VM、VM的port、SecurityGroup、ServerGroup都是不可见的。同时，一个loadbalancer的创建会占用租户subnet内的IP资源和port配额。

有了虚拟机后，会根据入参的subnet创建port，port的IP作为VIP。同时在这个subnet下给两个虚拟机分别挂载网卡，将VIP作为address pair配置到网卡。对这几个port配置相应的安全组规则。

> `allowed_address_pairs`特性，参见[这里](http://blog.aaronorosen.com/implementing-high-availability-instances-with-neutron-using-vrrp/).

然后，向虚拟机发送REST API消息（URL: plug/vip/{vip}），配置haproxy。

如果是active standby，在DB添加`vrrp_group`表记录，仍然通过REST调用，将keepalived配置文件下发到两个虚拟机中，并启动虚拟机中的keepalived服务。

至此，一个loadbalancer就创建结束了。基本上，后面创建listener、pool、member、health monitor，都是围绕这两个虚拟机，对haproxy和keepalived进程进行配置。

## 逻辑架构图
下图是通过octavia创建一个loadbalancer之后的逻辑图。  
![logical architecture of octavia loadbalancer](/images/2016-03-30-octavia/1.png)

octavia中有个叫health-monitor的进程，其任务之一是监听来自amphorae虚拟机发送的运行状态数据，以此更新lb、listener、pool、member的状态，最重要的是更新`amphora_health`表。同时，更新`listener_statistics`表，这个表的数据可以作为计费依据。

health-monitor进程的任务之二，是根据`amphora_health`表，找到异常状态的amphorae虚拟机，对该虚拟机进行更换操作。即删除旧的虚拟机，创建并配置新的amphorae虚拟机。

> 需要注意的是，health-monitor进程的监听IP和端口，必须在lb-mgmt-net内，以便接收amphorae的消息。可以查看devstack安装脚本了解该步骤的初始化过程。

octavia还有个进程叫house-keeping，主要任务是确保空闲的amphorae池大小、定期清理db中已删除的amphorae记录、定期更新amphorae中的证书。

## Fail over流程
health-monitor根据`amphora_health`表检测到有amphorae状态异常时，就会触发fail over流程。

1. 获取异常amphorae虚拟机的port信息（不考虑lb-mgmt-net上的port），将这些port的`device_id`设置为空
2. 删除amphorae虚拟机
3. 创建新的amphorae虚拟机并配置haproxy和keepalived

## 安装部署
Octavia要与Neutron配合，安装时neutron-lbaas软件包要与Neutron软件包安装在同一位置，在python路径中，neutron-lbaas目录与neutron目录同级。Octavia几个服务进程可以不与Neutron服务进程部署在同一host，因为neutron plugin与octavia是通过rest api通信。其实可以通过octavia中的devstack脚本了解其配置和安装过程。

当然，如果是生产环境要用，还要考虑其与周边安装工具的配合是否完备。比如是否有debian包、是否支持ansible安装、是否支持puppet安装？关于ansible的安装，社区正在做：

<https://specs.openstack.org/openstack/openstack-ansible-specs/specs/mitaka/lbaasv2.html>  
<https://blueprints.launchpad.net/openstack-ansible/+spec/lbaasv2>

上面的BP负责人回答：LBaaS v2 with agent (not octavia) is in Mitaka now, but the Liberty backport is still getting reviewed, octavia support is on hold until we can get something to make the LB mgmt network easier.

所以，截止Mitaka，如果要使用octavia，只有devstack是可用的。octavia社区正在完善installation文档。

在部署过程中，需要注意，octavia在设计之初仅考虑与neutron通信，目前缺少keystone认证机制，所以建议octavia api使用私有地址对neutron提供服务，并且建议octavia api进程与neutron server进程部署在一起。

## AWS Elastic Loadbalancing
分析任何OpenStack中项目时，都会不由自主的看一看老大哥AWS的类似服务的文档，毕竟OpenStack很多project都是照着AWS实现的。对于LBaaS来讲也不例外，AWS中就有一个Elastic Loadbalancing服务，参考如下几个链接：

<https://aws.amazon.com/elasticloadbalancing/>  
<http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/API_Welcome.html>   
<http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elastic-load-balancing.html>  

## FAQ
> 添加member时，如果参数中member所在的subnet与loadbalancer的subnet不一致，会怎样？  

Octavia允许member的IP地址范围与loadbalancer的subnet范围不同，内部实现时，会在member的IP地址范围内给amphora虚拟机添加网卡，以便与member通信。从另一种意义上来说，其实是对member所在subnet的IP地址空间的占用，而该subnet的租户是不感知的。

> 相似的问题，添加member时，如果member所在的subnet与lb-mgmt-net有地址重叠该怎么办？此时，amphorae会有多个网卡属于同一网段。

这个问题我也问过社区，目前有个[patch](https://review.openstack.org/#/c/300292/)正在解决这个问题。方法是在amphorae内使用namespace的方式对tenant network与lb-mgmt-net进行隔离。

> listener中的peer_port是干啥的？

TBA

> 创建member时，如果address不在subnet范围内，会怎么样？

我在阅读代码时发现了这个问题，并没有测试。按照代码逻辑，这个操作不会有什么异常。但loadbalancer肯定无法使用，因为amphora虚拟机根本访问不到member的IP地址。碰巧，我在review一个patch涉及此问题，我把问题抛给了patch的作者。patch链接：<https://review.openstack.org/292614>

> 如何升级amphora虚拟机的内核？

这个问题是团队内部考量是否部署octavia时，operator提出的一个很现实的问题。根据我对octavia的代码理解以及与octavia社区core的交流，我给出的答复如下：

There is a process called octavia-health-manager in octavia, it will check the amphora vm status and do failover. For example, we have amp1(master) and amp2(backup) for a loadbalancer in active-standby mode, when we want to patch the vm, we need to do following things:

1. we need to make a new image included patches we need using
diskimage-builder tool.
2. update image data in Glance.
3. Mark down the management port of any one of the two vms(amp2 is recommended).
4. the octavia-health-manager process will know that, and it will
replace the vm with a new one, using new image.
5. repeat for the other vm.

其实一开始我的答案中第三步是删除虚拟机，但octavia社区目前仍有一个相关的bug待解决，所以octavia社区建议down掉虚拟机的port即可。bug链接在[这里](https://bugs.launchpad.net/octavia/+bug/1509706)。

> 高版本的octavia可否与低版本的neutron(lbaas v2)兼容？

TBA

> octavia会依赖babican么？

在octavia的安装和使用中，我看到了octavia使用babican来做密钥管理。但如果不使用octavia提供的TLS Termination的功能，就可以不使用babican。
