---
layout: post
title: Octavia--Neutron中LBaaS的参考实现
description: Octavia--Neutron中LBaaS的参考实现
category: blog
---

声明：  
本博客欢迎转发，但请保留原作者信息!  
新浪微博：[@Lingxian_kong](http://weibo.com/lingxiankong)   
博客地址：<http://lingxiankong.github.io/>  
内容系本人学习、研究和总结，如有雷同，实属荣幸！

目标读者：OpenStack developer or operator

## Octavia简介
Octavia主要参与厂商：<http://stackalytics.com/?project_type=openstack&metric=commits&module=octavia>  
Octavia的开发者文档：<http://docs.octavia.io/review/master/>

你需要了解的前置知识：

- OpenStack :-)
- Haproxy和Keeplived。Octavia说白了，就是将用户的API请求经过逻辑处理，转换成haproxy和keepalived的配置参数，下发到amphorae虚拟机中。关于haproxy和keepalived，可以详细阅读各自的官方文档。同时，可以参考如下几篇博客：

    * 介绍haproxy+keepalive安装配置，写于06 Nov 2011：<http://weizhifeng.net/HA-with-HAProxy-and-KeepAlived.html>
    * haproxy互为主备, 2013-09-29：<http://502245466.blog.51cto.com/7559397/1303506>
    * Keepalived Check and Notify Scripts, February 21, 2014: <https://tobrunet.ch/2013/07/keepalived-check-and-notify-scripts/>

- TaskFlow。Octavia的内部实现中，逻辑流程的处理主要使用TaskFlow库。关于TaskFlow，请参见[官方文档](http://docs.openstack.org/developer/taskflow/)。

## 命令行使用流程
如下几个命令创建一个loadbalancer，一个listener一个pool，同时添加两个member。

    neutron lbaas-loadbalancer-create --name lb1 private-subnet
    neutron lbaas-listener-create --loadbalancer lb1 --protocol HTTP --protocol-port 80 --name listener1
    neutron lbaas-pool-create --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTP --name pool1
    neutron lbaas-member-create  --subnet private-subnet --address ${IP1} --protocol-port 80 pool1
    neutron lbaas-member-create  --subnet private-subnet --address ${IP2} --protocol-port 80 pool1
    
## Octavia与Neutron的关系
作为Neutron中LBaaS的参考实现，与其说Octavia与Neutron的关系，还不如说Octavia与Neutron中LBaaS service plugin的关系。之前有人在网上问过同样的问题，一位Octavia core reviewer给出如下答案：

lbaas v1:  
This is the original Neutron LBaaS, and what you see in Horizon or in the neutron CLI as “lb-*”. It has an haproxy backend, and a few vendors supporting it. Feature-wise, it’s basically a byte pump.

lbaas v2:  
This is the “new” Neutron LBaaS, and is in the neutron CLI as “lbaas-*” (it’s not yet in Horizon.) It first shipped in Kilo. It re-organizes the objects, and adds TLS termination support, and has L7 plus other new goodies planned in Liberty. It similarly has an haproxy reference backend with a few vendors supporting it.

octavia:  
Think of this as a service vm framework that is specific to lbaas, to implement lbaas via nova VMs instead of “lbaas agents". It is expected to be the reference backend implementation for neutron lbaasv2 in liberty. It could also be used as its own front-end, and/or given drivers to be a load balancing framework completely outside neutron/nova, though that is not the present direction of development.

总结一下就是，Neutron中有LBaaS plugin，在LBaaS plugin中有很多provider或者叫driver（比如开源的haproxy driver或其他很多网络设备厂家实现的driver），其中一个叫octavia driver，该driver做的事情就是调用Octavia的API，与Octavia服务交互。

还有一点值得注意，使用Octavia时，在Neutron和Octavia中都会有database，记录同样的信息。Octavia只是作为LBaaS的service provider之一，所以，Neutron DB中同样会记录loadbalancer与provider的对应关系。我问过一个core reviewer，如何保证这两份数据的一致性，他没有正面回答我，只说他也觉得这样的设计不妥，其实用Octavia一个DB即可，neutron中可以只记录mapping，但因为'political'的原因，不得不如此。因此，在生产环境，建议自己实现一致性检查。

## Octavia与其他OpenStack服务的交互

### Nova
创建虚拟机：  

- a pre-defined image(id or tag), containing the haproxy software.
- a pre-defined security group.
- a pre-defined private network.
- use `config_drive` or `user_data` to transfer haproxy software configurations.

### Glance
查询Glance镜像（可选）：  
如果在配置文件中指定了`amp_image_id`，则不会跟Glance交互，直接传递给Nova创建虚拟机；如果指定了`amp_image_tag`，则会从Glance中取出最新的image。

### Neutron
创建/查询port信息

安全组：  
为loadbalancer创建安全组及规则，名称格式`'lb-<loadbalancer-id>'`，并把安全组作用到loadbalancer虚拟机的port上。

使用了`allowed_address_pairs`特性，参见[这里](http://blog.aaronorosen.com/implementing-high-availability-instances-with-neutron-using-vrrp/).

## 创建loadbalancer流程

目前支持single和active standby两种模式的loadbalancer，通过配置文件配置。测试环境可以是single，但生产环境使用时还是建议active standby。社区正在开发active active，目前（Mitaka）暂不支持。这里以active standby模式为例。

创建loadbalancer，Octavia会创建两个虚拟机。如果配置`enable_anti_affinity`，则会先在Nova创建ServerGroup（这个ServerGroup的ID会记录在DB中），两个虚拟机就会创建在不同的host上。虚拟机的flavor、image、network、keypair信息都是从配置文件中获取，其中network就是Octavia进程与虚拟机通信的管理平面。

> 这里有个问题，Octavia每创建一个loadbalancer，都会创建一个ServerGroup，有这个必要么？要考虑Nova中ServerGroup的配额与loadbalancer配额的对应。这个ServerGroup的租户是谁？虚拟机的租户又是谁？admin？loadbalancer中会有project id么？从lbaas plugin中传入的参数中没有project id。

有了虚拟机后，会根据入参的subnet创建port，port的IP作为VIP。同时在这个subnet下给两个虚拟机分别挂载网卡，将VIP作为address pair配置到网卡。对这几个port配置相应的安全组规则。

然后，向虚拟机发送REST API消息（URL: plug/vip/{vip}），配置haproxy。

如果是active standby，在DB添加`vrrp_group`表记录，仍然通过REST调用，将keepalived配置文件下发到两个虚拟机中，并启动虚拟机中的keepalived服务。

至此，一个loadbalancer就创建结束了。基本上，后面创建listener、pool、member、health monitor，都是围绕这两个虚拟机，对haproxy和keepalived进行配置。

## 安装
<https://specs.openstack.org/openstack/openstack-ansible-specs/specs/mitaka/lbaasv2.html>

## AWS Elastic Loadbalancing
分析任何OpenStack中项目时，都会不由自主的看一看老大哥AWS的类似服务的文档，毕竟OpenStack很多project都是照着AWS实现的。对于LBaaS来讲也不例外，AWS中就有一个Elastic Loadbalancing服务，参考如下几个链接：

<https://aws.amazon.com/elasticloadbalancing/>
<http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/API_Welcome.html>  
<http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elastic-load-balancing.html>  

## FAQ
> 创建member时，如果参数中member的address不在loadbalancer的subnet内会怎样？  

Octavia允许member的IP地址范围与loadbalancer的subnet范围不同，内部实现时，会在member的IP地址范围内给amphora虚拟机添加网卡，以便与member通信。从另一种意义上来说，其实是对member所在subnet配额的占用。作为subnet的租户，知道么？
